"""Fila de alertas com deduplica√ß√£o e rate limiting."""

import asyncio
import hashlib
import logging
from datetime import datetime, timedelta
from typing import Optional

from src.domain.entities.alerta import AlertaOportunidade

logger = logging.getLogger(__name__)


class FilaAlertas:
    """
    Queue de alertas com garantias de entrega e deduplica√ß√£o.

    Garantias:
    - Ordem preservada (FIFO)
    - Deduplica√ß√£o >95%: 1 alerta/padr√£o/minuto
    - Max 3 alertas simult√¢neos (backpressure)
    - Sem perda de dados (append-only em auditoria)
    - Rate limiting STRICT

    Internamente:
    - asyncio.Queue para ordena√ß√£o
    - Dict para dedup cache com TTL
    - Lock para thread-safety
    """

    def __init__(
        self,
        max_queue_size: int = 100,
        rate_limit_seconds: int = 60,
        dedup_ttl_seconds: int = 120,
    ):
        """
        Inicializa fila.

        Args:
            max_queue_size: M√°ximo de alertas na fila
            rate_limit_seconds: Segundos entre alertas do mesmo padr√£o
            dedup_ttl_seconds: TTL do cache de deduplica√ß√£o
        """
        self.fila = asyncio.Queue(maxsize=max_queue_size)
        self.lock = asyncio.Lock()

        # Rate limiting: {padr√£o: timestamp_√∫ltimo_alerta}
        self.rate_limiter = {}

        # Dedup cache: {hash: timestamp}
        # Entradas expiram ap√≥s dedup_ttl_seconds
        self.dedup_cache = {}

        # Alertas em processamento (para backpressure)
        self.em_processamento = set()

        self.rate_limit_seconds = rate_limit_seconds
        self.dedup_ttl_seconds = dedup_ttl_seconds

        self.metrics = {
            "total_enfileirados": 0,
            "total_duplicados": 0,
            "total_rate_limited": 0,
            "total_processados": 0,
            "falhas": 0,
        }

    async def enfileirar(self, alerta: AlertaOportunidade) -> bool:
        """
        Enfileira alerta com deduplica√ß√£o e rate limiting.

        L√≥gica STRICT:
        1. Verifica rate limit (m√°x 1/padr√£o/minuto)
        2. Verifica deduplica√ß√£o (hash do alerta)
        3. Se passa: enfileira
        4. Se falha rate limit ou dedup: retorna False (n√£o enfileira)

        Args:
            alerta: AlertaOportunidade a enfileirar

        Returns:
            True se enfileirado com sucesso
            False se duplicado/rate-limited
        """

        async with self.lock:
            agora = datetime.now()

            # PASSO 1: Verifica rate limiting
            ultima_vez = self.rate_limiter.get(alerta.padrao.value)

            if ultima_vez:
                elapsed = (agora - ultima_vez).total_seconds()
                if elapsed < self.rate_limit_seconds:
                    logger.debug(
                        f"Rate limit: {alerta.padrao.value} ainda em cooldown "
                        f"({elapsed:.1f}s < {self.rate_limit_seconds}s)"
                    )
                    self.metrics["total_rate_limited"] += 1
                    return False

            # PASSO 2: Verifica deduplica√ß√£o (hash do alerta)
            hash_alerta = self._calcular_hash(alerta)

            if hash_alerta in self.dedup_cache:
                timestamp_cache = self.dedup_cache[hash_alerta]
                idade = (agora - timestamp_cache).total_seconds()

                if idade < self.dedup_ttl_seconds:
                    logger.debug(
                        f"Dedup: alerta {alerta.id} √© duplicado "
                        f"(idade {idade:.1f}s, TTL {self.dedup_ttl_seconds}s)"
                    )
                    self.metrics["total_duplicados"] += 1
                    return False
                else:
                    # Cache expirou, remove
                    del self.dedup_cache[hash_alerta]

            # PASSO 3: Backpressure check (m√°x 3 simult√¢neos)
            if len(self.em_processamento) > 3:
                logger.warning(
                    f"Fila com backpressure: {len(self.em_processamento)} "
                    f"alertas em processamento"
                )
                # Pode continuar enfileirando, mas log de aviso

            # PASSO 4: Enfileira
            try:
                self.fila.put_nowait((alerta, agora))

                # Atualiza rate limiter e dedup
                self.rate_limiter[alerta.padrao.value] = agora
                self.dedup_cache[hash_alerta] = agora

                logger.info(
                    f"‚úÖ Alerta enfileirado: {alerta.id} "
                    f"(padr√£o: {alerta.padrao.value})"
                )
                self.metrics["total_enfileirados"] += 1
                alerta.marcar_enfileirado()

                return True

            except asyncio.QueueFull:
                logger.error(
                    "‚ùå Fila de alertas cheia! Aumentar max_queue_size"
                )
                self.metrics["falhas"] += 1
                return False

    async def processar_fila(
        self, delivery_manager
    ) -> None:
        """
        Worker que processa alertas da fila.

        Executa indefinidamente, pegando alertas e entregando.

        Args:
            delivery_manager: AlertaDeliveryManager para entrega
        """

        logger.info("üöÄ Worker de processamento iniciado")

        while True:
            try:
                # Pega alerta da fila (bloqueia se vazia)
                alerta, timestamp_enfileirado = await self.fila.get()

                try:
                    # Registra que est√° processando
                    self.em_processamento.add(alerta.id)

                    # Entrega (async, n√£o bloqueia)
                    await delivery_manager.entregar_alerta(alerta)

                    self.metrics["total_processados"] += 1

                    logger.info(f"‚úÖ Alerta processado: {alerta.id}")

                finally:
                    # Remove do set de processamento
                    self.em_processamento.discard(alerta.id)
                    self.fila.task_done()

            except Exception as e:
                logger.error(f"‚ùå Erro ao processar alerta: {e}")
                self.metrics["falhas"] += 1
                await asyncio.sleep(1)  # Backoff on error

    def _calcular_hash(self, alerta: AlertaOportunidade) -> str:
        """
        Calcula hash do alerta para deduplica√ß√£o.

        Ignora:
        - Timestamp (alertas pr√≥ximos s√£o "iguais")
        - ID (cada inst√¢ncia √© √∫nica)

        Considera:
        - Ativo
        - Padr√£o
        - Pre√ßo atual (com toler√¢ncia 0.5% = mesma vela)

        Args:
            alerta: AlertaOportunidade

        Returns:
            Hash SHA256 (primeiros 16 chars)
        """

        # Arredonda pre√ßo pra 0.5% (detec√ß√£o de mesma vela)
        preco_tolerance = float(alerta.preco_atual) * 0.005
        preco_rounded = (
            int(float(alerta.preco_atual) / preco_tolerance) * preco_tolerance
        )

        dados = (
            f"{alerta.ativo}|"
            f"{alerta.padrao.value}|"
            f"{preco_rounded:.2f}"
        )

        hash_completo = hashlib.sha256(dados.encode()).hexdigest()
        return hash_completo[:16]  # Primeiros 16 chars

    def obter_metricas(self) -> dict:
        """
        Retorna m√©tricas da fila (debug e monitoramento).

        Returns:
            Dict com m√©tricas de performance
        """
        return {
            **self.metrics,
            "tamanho_fila_atual": self.fila.qsize(),
            "em_processamento": len(self.em_processamento),
            "rate_limiter_size": len(self.rate_limiter),
            "dedup_cache_size": len(self.dedup_cache),
        }

    async def limpar_cache_expirado(self) -> None:
        """
        Limpa cache de deduplica√ß√£o expirado periodicamente.

        Pode ser executado em background a cada 60 segundos.
        """

        while True:
            try:
                await asyncio.sleep(60)  # Executa a cada 60 segundos

                agora = datetime.now()
                chaves_expiradas = []

                async with self.lock:
                    for hash_alerta, timestamp in self.dedup_cache.items():
                        idade = (agora - timestamp).total_seconds()
                        if idade > self.dedup_ttl_seconds:
                            chaves_expiradas.append(hash_alerta)

                    for chave in chaves_expiradas:
                        del self.dedup_cache[chave]

                    if chaves_expiradas:
                        logger.debug(
                            f"Limpeza de cache: {len(chaves_expiradas)} "
                            f"entradas expiradas removidas"
                        )

            except Exception as e:
                logger.error(f"Erro ao limpar cache: {e}")
                await asyncio.sleep(5)  # Retry em 5 segundos

    def resetar_metricas(self) -> dict:
        """
        Reseta m√©tricas e retorna valores anteriores.

        √ötil para coleta de m√©tricas peri√≥dicas.

        Returns:
            M√©tricas antes do reset
        """
        metricas_anterior = self.metrics.copy()

        self.metrics = {
            "total_enfileirados": 0,
            "total_duplicados": 0,
            "total_rate_limited": 0,
            "total_processados": 0,
            "falhas": 0,
        }

        return metricas_anterior
